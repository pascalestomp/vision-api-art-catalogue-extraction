{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11549d15-af36-4fce-8753-fa13cf093e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\avamj\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\avamj\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: openai in c:\\users\\avamj\\anaconda3\\lib\\site-packages (1.65.4)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\avamj\\anaconda3\\lib\\site-packages (1.25.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avamj\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas tqdm openai PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ef68e8-c338-47e6-abfe-f0c4968f46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from io import StringIO\n",
    "import fitz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f5b786-6a68-4518-a020-c9f44e31dc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÅ Processing PDFs:   0%|                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing PDF: 201501635.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 26: expected 13 fields, saw 14\n",
      "Skipping line 27: expected 13 fields, saw 14\n",
      "Skipping line 28: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 6: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 47: expected 13 fields, saw 16\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 3: expected 13 fields, saw 14\n",
      "Skipping line 9: expected 13 fields, saw 14\n",
      "Skipping line 16: expected 13 fields, saw 14\n",
      "Skipping line 18: expected 13 fields, saw 14\n",
      "Skipping line 20: expected 13 fields, saw 15\n",
      "Skipping line 21: expected 13 fields, saw 14\n",
      "Skipping line 22: expected 13 fields, saw 14\n",
      "Skipping line 30: expected 13 fields, saw 16\n",
      "Skipping line 34: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 18: expected 13 fields, saw 14\n",
      "Skipping line 21: expected 13 fields, saw 14\n",
      "Skipping line 34: expected 13 fields, saw 14\n",
      "Skipping line 39: expected 13 fields, saw 14\n",
      "Skipping line 44: expected 13 fields, saw 14\n",
      "Skipping line 45: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 10: expected 13 fields, saw 15\n",
      "Skipping line 19: expected 13 fields, saw 14\n",
      "Skipping line 21: expected 13 fields, saw 14\n",
      "Skipping line 22: expected 13 fields, saw 14\n",
      "Skipping line 23: expected 13 fields, saw 14\n",
      "Skipping line 37: expected 13 fields, saw 14\n",
      "Skipping line 41: expected 13 fields, saw 14\n",
      "Skipping line 51: expected 13 fields, saw 14\n",
      "Skipping line 52: expected 13 fields, saw 14\n",
      "Skipping line 53: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 13: expected 13 fields, saw 14\n",
      "Skipping line 14: expected 13 fields, saw 14\n",
      "Skipping line 24: expected 13 fields, saw 14\n",
      "Skipping line 25: expected 13 fields, saw 14\n",
      "Skipping line 29: expected 13 fields, saw 15\n",
      "Skipping line 37: expected 13 fields, saw 14\n",
      "Skipping line 42: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 11: expected 13 fields, saw 14\n",
      "Skipping line 26: expected 13 fields, saw 14\n",
      "Skipping line 29: expected 13 fields, saw 14\n",
      "Skipping line 31: expected 13 fields, saw 14\n",
      "Skipping line 32: expected 13 fields, saw 14\n",
      "Skipping line 38: expected 13 fields, saw 15\n",
      "Skipping line 45: expected 13 fields, saw 14\n",
      "Skipping line 46: expected 13 fields, saw 16\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "C:\\Users\\AvaMJ\\AppData\\Local\\Temp\\ipykernel_28164\\1143848146.py:94: ParserWarning: Skipping line 12: expected 13 fields, saw 14\n",
      "Skipping line 15: expected 13 fields, saw 14\n",
      "Skipping line 46: expected 13 fields, saw 14\n",
      "\n",
      "  df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
      "üìÅ Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [28:55<00:00, 1735.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extraction complete! Data saved to deepseek_CHextracted3.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize DeepSeek client\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the DEEPSEEK_API_KEY environment variable.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com\"  # DeepSeek API base URL\n",
    ")\n",
    "# Define Excel columns\n",
    "COLUMNS = [\n",
    "    \"Catalogue Identifier\", \"Keyword Person\", \"Artist City\",\n",
    "    \"Artist Address\", \"Artist Abbreviations\", \"Entry Number\",\n",
    "    \"Free Title\", \"Additional Artwork Info\", \"Asterisk\",\n",
    "    \"Amount Type\", \"Currency\", \"Price\", \"Full Entry Quote\"\n",
    "]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text directly from a PDF file using PyMuPDF.\"\"\"\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()  # Extract text from each page\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def format_text_into_table(raw_text, pdf_name):\n",
    "    \"\"\"Formats extracted text into structured table using DeepSeek.\"\"\"\n",
    "    catalogue_identifier = pdf_name\n",
    "    extracted_data = []\n",
    "    chunk_size = 3000  # Limit characters per request to avoid API truncation\n",
    "\n",
    "    # Split the raw text into manageable chunks\n",
    "    text_chunks = [raw_text[i:i + chunk_size] for i in range(0, len(raw_text), chunk_size)]\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        try:\n",
    "            prompt = f\"\"\"Convert this Dutch exhibition catalog text into a CSV table:\n",
    "            {chunk}\n",
    "\n",
    "            Rules:\n",
    "            - **Catalogue Identifier:** Use `{catalogue_identifier}` for every row.\n",
    "            - **Artist Name:** Extract the surname first, then first name. If written like:\n",
    "            - \"VAN GOGH, J. H.\" ‚Üí Convert to \"J. H. Van Gogh\"\n",
    "            - \"VAN GOGH (J. H.)\" ‚Üí Convert to \"J. H. Van Gogh\"\n",
    "            - **Location:** If separated by a comma, extract before and after the comma as:\n",
    "            - First part (before comma) ‚Üí `Artist City`\n",
    "            - Second part (after comma) ‚Üí `Artist Address`\n",
    "            - **Artist Abbreviations:** Extract if present (e.g., \"GL, BL, KL, BM\"). these are usually found after the artist name or location\n",
    "            - do not confuse abbreviations with initials\n",
    "            - **Entry Number:** Extract the **actual number appearing before each line**.\n",
    "            - **Free Title:** Extract the artwork title after the entry number.\n",
    "            - **Additional Artwork Info:** If parentheses appear next to the title, store their content here.\n",
    "            - **Asterisk:** If an asterisk (`*`) appears, store `\"true\"`, otherwise `\"false\"`.\n",
    "            - **Currency & Pricing:**\n",
    "            - **Full Entry Quote:** Copy the full text of the entry exactly as it appears. Including the related artist information.\n",
    "            - If a price list is detected, match the entry number to its corresponding price.\n",
    "            - Extract currency symbols (∆í, gulden, fl, m, mark, mrk, etc.).\n",
    "            - If price is `\"\"`, inherit the most recent currency.\n",
    "            - If no price list, store `\"unknown\"` for Amount Type, Currency, and Price.\n",
    "            - fill empty cells with \"unknown\"\n",
    "            - assume entries like 'idem', 'deselfde', 'dezelfde', 'dito', '\"' mean same as above.\n",
    "            - assume Amount Type is always \"Asking price\"\n",
    "            - correct typos automatically\n",
    "            - Output **only** the CSV table. No explanations or formatting outside of CSV.\n",
    "            - Always include header row: {','.join(COLUMNS)}\n",
    "            \"\"\"\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a data formatting assistant. Output CSV data only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=4000,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            if response.choices[0].message.content:\n",
    "                formatted_data = response.choices[0].message.content.strip()\n",
    "\n",
    "                # Optional: Clean unexpected lines (defensive programming)\n",
    "                csv_lines = [line for line in formatted_data.splitlines() if line.strip() and not line.lower().startswith(\"here is\")]\n",
    "                formatted_data_cleaned = \"\\n\".join(csv_lines)\n",
    "\n",
    "                # Optional: Save raw output for debugging\n",
    "                # with open(f\"{pdf_name}_deepseek_output_chunk.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "                #     f.write(formatted_data_cleaned + \"\\n\")\n",
    "\n",
    "                df_chunk = pd.read_csv(StringIO(formatted_data_cleaned), delimiter=\",\", names=COLUMNS, quotechar='\"', on_bad_lines=\"warn\")\n",
    "\n",
    "                extracted_data.append(df_chunk)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error formatting text chunk from {pdf_name}: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames from chunks\n",
    "    if extracted_data:\n",
    "        return pd.concat(extracted_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=COLUMNS)\n",
    "\n",
    "def process_pdfs(base_folder):\n",
    "    \"\"\"Processes all PDFs in a folder and extracts structured data.\"\"\"\n",
    "    extracted_data = []\n",
    "\n",
    "    for pdf_file in tqdm(os.listdir(base_folder), desc=\"üìÅ Processing PDFs\"):\n",
    "        if pdf_file.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(base_folder, pdf_file)\n",
    "            print(f\"\\nüìÑ Processing PDF: {pdf_file}\")\n",
    "\n",
    "            # Extract raw text from PDF\n",
    "            raw_text = extract_text_from_pdf(pdf_path)\n",
    "            if not raw_text:\n",
    "                print(f\"‚ö†Ô∏è No text extracted from {pdf_file}\")\n",
    "                continue\n",
    "\n",
    "            # Format text into structured table\n",
    "            df_entries = format_text_into_table(raw_text, pdf_file)\n",
    "            if not df_entries.empty:\n",
    "                extracted_data.append(df_entries)\n",
    "\n",
    "    # Combine all data into one DataFrame\n",
    "    final_df = pd.concat(extracted_data, ignore_index=True) if extracted_data else pd.DataFrame(columns=COLUMNS)\n",
    "\n",
    "    # Save extracted data to Excel\n",
    "    if not final_df.empty:\n",
    "        output_file = \"deepseek_CHextracted3.xlsx\"\n",
    "        final_df.to_excel(output_file, index=False)\n",
    "        print(f\"\\n‚úÖ Extraction complete! Data saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"‚ùå No data extracted! Check API responses.\")\n",
    "\n",
    "# Run the process\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_FOLDER = r\"C:\\Users\\AvaMJ\\Downloads\\LM20250207\\LM20250207pdf4\\pdf4test\"\n",
    "    process_pdfs(BASE_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbf3be-e35a-44cf-8122-54447f8a59ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57e9c4-77b6-4d18-ad5b-bf0ce42c41a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
